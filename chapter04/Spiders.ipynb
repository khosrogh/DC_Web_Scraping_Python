{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bead133-bfce-4513-b3ba-f3cf1beb1a4f",
   "metadata": {},
   "source": [
    "### Inheriting the Spider\r\n",
    "When learning about scrapy spiders, we saw that the main portion of the code for us to adjust is the class for the spider. To help build some familiarity of the class, you will complete a short piece of code to complete a toy-model of the spider class code. We've omitted the code that would actually run the spider, only including the pieces necessary to create the class.\r\n",
    "\r\n",
    "As mentioned in the lesson, a class is roughly a collection of related variables and functions housed together. Sometimes one class likes to use methods from another class, and so we will inherit methods from a different class. That's what we do in the spider class.\r\n",
    "\r\n",
    "We wrote the function inspect_class to look at the your class once you're done, if you'd like to test your solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ab715-70bb-4aca-8eec-78e59de6df2a",
   "metadata": {},
   "source": [
    "Pass scrapy.Spider as an argument to the class YourSpider; this will make it so that YourSpider inherits the methods from scrapy.Spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097e84bd-04b8-424b-b387-cdbfaaa68258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your spider class name is: your_spider\n",
      "It seems you have inherited methods from scrapy.Spider -- NICE!\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    pass\n",
    "  # parse method\n",
    "  def parse(self, response):\n",
    "    pass\n",
    "\n",
    "def inspect_class(c):\n",
    "  newc = c()\n",
    "  meths = dir(newc)\n",
    "  if 'name' in meths:\n",
    "    print(\"Your spider class name is:\", newc.name)\n",
    "  if 'from_crawler' in meths:\n",
    "    print(\"It seems you have inherited methods from scrapy.Spider -- NICE!\")\n",
    "  else:\n",
    "    print(\"Oh no! It doesn't seem that you are inheriting the methods from scrapy.Spider!!\")\n",
    "      \n",
    "# Inspect Your Class\n",
    "inspect_class(YourSpider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20867828-16e8-487a-904d-f7c1d3a016fb",
   "metadata": {},
   "source": [
    "### Hurl the URLs\r\n",
    "In the next lesson we will talk about the start_requests method within the spider class. In this quick exercise, we ask you to change around a variable within the start_requests method which foreshadows some of what we will be learning in the next lesson. Basically, we want you to start becoming comfortable turning some of the wheels within a spider class; in this case, making a list of urls within the start_requests method.\r\n",
    "\r\n",
    "We've written a function inspect_class which will print out the list of elements you have in the urls variable within the start_requests method.\r\n",
    "\r\n",
    "Note: in the next several exercises, you will write code to complete your spider class, but the code does not yet include the pieces to actually run the spider; that will come at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f82ac8d-1650-4666-9106-1bc440db5039",
   "metadata": {},
   "source": [
    "Fill in the blank within the start_requests method to assign the variable urls a list with the two strings: \"https://www.datacamp.com\" and\"https://scrapy.org\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7300031b-476f-43ed-8de9-a4451bcb9754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your spider class name is: your_spider\n",
      "It seems you have inherited methods from scrapy.Spider -- NICE!\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    urls = [\"https://www.datacamp.com\", \"https://scrapy.org\"]\n",
    "    for url in urls:\n",
    "      yield url\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4cae5-a81b-4a7d-902a-0ec9a42c2eaf",
   "metadata": {},
   "source": [
    "### Self Referencing is Classy\r\n",
    "You probably have noticed that within the spider class, we always input the argument self in the start_requests and parse methods (just look in the sample code in this exercise!). This allows us to reference between methods within the class. That is, if we want to refer to the method parse within the start_requests method, we would need to write self.parse rather than just parse; what writing self does is tell the code: \"Look in the same class as start_requests for a method called parse to use.\"\r\n",
    "\r\n",
    "In this exercise you will get a chance to play with this \"self referencing\".\r\n",
    "\r\n",
    "Instruct100 XP\r\n",
    "Fill in the required scrapy object into the class YourSpider needed to create the scrapy s\n",
    "\n",
    "pider.\r\n",
    "Pass the string argument \"Hello World!\" to fill in the blank in the start_requests method to use the print_msg method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "416f94c3-390f-4299-bb8e-a428ea021c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your spider class name is: your_spider\n",
      "It seems you have inherited methods from scrapy.Spider -- NICE!\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    self.print_msg( \"Hello World!\" )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  # print_msg method\n",
    "  def print_msg( self, msg ):\n",
    "    print( \"Calling start_requests in YourSpider prints out:\", msg )\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00270c99-3d72-4b90-812d-a27107c69f7d",
   "metadata": {},
   "source": [
    "Self referencing in classes can be a bit confusing, but you nailed it!! (And, don't worry, we don't need to worry too much more about this subject for this course)!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d4b395-5900-41a6-9ebd-d842e248a8ee",
   "metadata": {},
   "source": [
    "### Starting with Start Requests\r\n",
    "In the last lesson we learned about setting up the start_requests method within a scrapy spider. Here we have another toy-model spider which doesn't actually scrape anything, but gives you a chance to play with the start_requests method. What we want is for you to start becomming familiar with the arguments you pass into the scrapy.Request call within start_requests.\r\n",
    "\r\n",
    "As before, we have created the function inspect_class to examine what you are yielding in start_requestswebsite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29970b6-a6f4-47a2-a9a1-fcd83acdf0ae",
   "metadata": {},
   "source": [
    "Fill in the required scrapy object into the class YourSpider needed to create the scrapy spider.\r\n",
    "\n",
    "\n",
    "Fill in the blank in the yielded scrapy.Request call within the start_requests method so that the URL this spider would start scraping is \"https://www.datacamp.com\" and would use the parse method (within the YourSpider class) as the method to parse the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7be0e003-c3c7-4f9c-b673-fb1e8ab20ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your spider class name is: your_spider\n",
      "It seems you have inherited methods from scrapy.Spider -- NICE!\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url=\"https://www.datacamp.com\", callback=self.parse )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4e5c14-ca6b-4ef3-a9a7-f89716d6833e",
   "metadata": {},
   "source": [
    "### Pen Names\r\n",
    "In this exercise, we have set up a spider class which, when finished, will retrieve the author names from a shortened version of the DataCamp course directory. The URL for the shortened version is stored in the variable url_short. Your job will be to create the list of extracted author names in the parse method of the spider.\r\n",
    "\r\n",
    "Two things you should know:\r\n",
    "\r\n",
    "You will be using the response object and the css method here.\r\n",
    "The course author names are defined by the text within the paragraph p elements belonging to the class course-block__author-name\r\n",
    "You can inspect the spider using the function inspect_spider() that we built for you -- it will print out the author names you find!\r\n",
    "\r\n",
    "Note that this and the remaining exercises in this chapter may take some time to load."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67fe08-16a5-4642-b3cc-55c5fd92fa64",
   "metadata": {},
   "source": [
    "Fill in the required arguments to the parse method so that it will work as required when called in the start_requests method.\r\n",
    "\n",
    "\n",
    "Within the parse method, create a variable author_names, which is a list of strings created by extracting the text from the paragraph elements belonging to the class course-block__author-name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17055752-8fd1-422f-9b57-52b6a795acf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh no! Something went wrong with the code. Keep trying!\n"
     ]
    }
   ],
   "source": [
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "\n",
    "def inspect_spider( s ):\n",
    "  news = s()\n",
    "  try:\n",
    "    req = list( news.start_requests() )[0]\n",
    "    url = req.url\n",
    "    html = requests.get( url ).content\n",
    "    response = TextResponse( url = url, body = html, encoding = 'utf-8' )\n",
    "    author_names = req.callback( response )\n",
    "    print( 'You have collected the author names:')\n",
    "    for a in author_names:\n",
    "      print('\\t-', a )\n",
    "  except:\n",
    "    print( 'Oh no! Something went wrong with the code. Keep trying!')\n",
    "\n",
    "\n",
    "# Create the Spider class\n",
    "class DCspider( scrapy.Spider ):\n",
    "  name = 'dcspider'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    # Create an extracted list of course author names\n",
    "    author_names = response.css('p.course-block__author-name::text').extract()\n",
    "    # Here we will just return the list of Authors\n",
    "    return author_names\n",
    "  \n",
    "# Inspect the spider\n",
    "inspect_spider( DCspider )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f95097-b034-4c58-aff4-6c111bc9bd90",
   "metadata": {},
   "source": [
    "### Crawler Time\r\n",
    "This will be your first chance to play with a spider which will crawl between sites (by first collecting links from one site, and following those links to parse new sites). This spider starts at the shortened DataCamp course directory, then extracts the links of the courses in the parse method; from there, it will follow those links to extract the course descriptions from each course page in the parse_descr method, and put these descriptions into the list course_descrs. Your job is to complete the code so that the spider runs as desired!\r\n",
    "\r\n",
    "We have created a function inspect_spider which will print out one of the course descriptions you scrape (if done correctly)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab130b5-958c-4600-8601-4d386ded9917",
   "metadata": {},
   "source": [
    "Fill in the two blanks below (one in each of the parsing methods) with the appropriate entries so that the spider can move from the first parsing method to the second correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965316d2-a1d5-463a-ad5f-4fabc9e91597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh no! Something went wrong with the code. Keep trying!\n"
     ]
    }
   ],
   "source": [
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the Spider class\n",
    "class DCdescr( scrapy.Spider ):\n",
    "  name = 'dcdescr'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  \n",
    "  # First parse method\n",
    "  def parse( self, response ):\n",
    "    links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
    "    # Follow each of the extracted links\n",
    "    for link in links:\n",
    "      yield response.follow(url=link, callback=self.parse_descr)\n",
    "      \n",
    "  # Second parsing method\n",
    "  def parse_descr( self, response ):\n",
    "    # Extract course description\n",
    "    course_descr = response.css( 'p.course__description::text' ).extract_first()\n",
    "    # For now, just yield the course description\n",
    "    yield course_descr\n",
    "\n",
    "\n",
    "# Inspect the spider\n",
    "inspect_spider( DCdescr )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e1a738-0152-4ea5-815c-aa128a8ac6c5",
   "metadata": {},
   "source": [
    "### Time to Run\n",
    "In the last lesson, we went through creating an entire web-crawler to access course information from each course in the DataCamp course directory. However, the lesson seemed to stop without a climax, because we didn't play with the code after finishing the parsing methods.\n",
    "\n",
    "The point of this exercise is to remedy that!\n",
    "\n",
    "The code we give you to look at in this and the next exercise is long, because its the entire spider that took us the lesson to create! However, don't be intimidated! The point of these two exercises is to give you a very easy task to complete, with the hope that you will look at and run the code for this spider. That way, even though it is long, you will have a grasp of it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b839567e-62a6-4b13-953e-088483db54ab",
   "metadata": {},
   "source": [
    "Fill in the one blank at the end of the parse_pages methods to assign the chapter titles to the dictionary whose key is the corresponding course title.\r\n",
    "\n",
    "\n",
    "NOTE: If you hit Run Code, you must Reset to Sample Code to successfully use Run Code again!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8f201da-cc3d-41f8-ab4b-a3e53389c990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-20 16:38:12 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: scrapybot)\n",
      "2024-01-20 16:38:12 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.12.1 (tags/v3.12.1:2305ca5, Dec  7 2023, 22:03:25) [MSC v.1937 64 bit (AMD64)], pyOpenSSL 23.3.0 (OpenSSL 3.1.4 24 Oct 2023), cryptography 41.0.7, Platform Windows-11-10.0.22631-SP0\n",
      "2024-01-20 16:38:12 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-01-20 16:38:12 [py.warnings] WARNING: D:\\DataCamp Courses\\Web Scraping in Python\\venv\\Lib\\site-packages\\scrapy\\utils\\request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2024-01-20 16:38:12 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-01-20 16:38:12 [scrapy.extensions.telnet] INFO: Telnet Password: b8fe25301937252c\n",
      "2024-01-20 16:38:12 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-01-20 16:38:12 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2024-01-20 16:38:12 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-01-20 16:38:12 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-01-20 16:38:12 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-01-20 16:38:12 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-01-20 16:38:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-01-20 16:38:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m process \u001b[38;5;241m=\u001b[39m CrawlerProcess()\n\u001b[0;32m     35\u001b[0m process\u001b[38;5;241m.\u001b[39mcrawl(DC_Chapter_Spider)\n\u001b[1;32m---> 36\u001b[0m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreviewCourses\u001b[39m( dc_dict, n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m ):\n\u001b[0;32m     43\u001b[0m   crs_titles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m( dc_dict\u001b[38;5;241m.\u001b[39mkeys() )\n",
      "File \u001b[1;32mD:\\DataCamp Courses\\Web Scraping in Python\\venv\\Lib\\site-packages\\scrapy\\crawler.py:427\u001b[0m, in \u001b[0;36mCrawlerProcess.start\u001b[1;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[0;32m    425\u001b[0m tp\u001b[38;5;241m.\u001b[39madjustPoolsize(maxthreads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mgetint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREACTOR_THREADPOOL_MAXSIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    426\u001b[0m reactor\u001b[38;5;241m.\u001b[39maddSystemEventTrigger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop)\n\u001b[1;32m--> 427\u001b[0m \u001b[43mreactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\DataCamp Courses\\Web Scraping in Python\\venv\\Lib\\site-packages\\twisted\\internet\\base.py:1317\u001b[0m, in \u001b[0;36m_SignalReactorMixin.run\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, installSignalHandlers: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartRunning\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmainLoop()\n",
      "File \u001b[1;32mD:\\DataCamp Courses\\Web Scraping in Python\\venv\\Lib\\site-packages\\twisted\\internet\\base.py:1299\u001b[0m, in \u001b[0;36m_SignalReactorMixin.startRunning\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;124;03mExtend the base implementation in order to remember whether signal\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;124;03mhandlers should be installed later.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;124;03m    installed during startup.\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_installSignalHandlers \u001b[38;5;241m=\u001b[39m installSignalHandlers\n\u001b[1;32m-> 1299\u001b[0m \u001b[43mReactorBase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartRunning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mReactorBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\DataCamp Courses\\Web Scraping in Python\\venv\\Lib\\site-packages\\twisted\\internet\\base.py:843\u001b[0m, in \u001b[0;36mReactorBase.startRunning\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    841\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorAlreadyRunning()\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_startedBefore:\n\u001b[1;32m--> 843\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorNotRestartable()\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_started \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    ch_titles = response.css('h4.chapter__title::text')\n",
    "    ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "    dc_dict[ crs_title_ext ] = ch_titles_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Chapter_Spider)\n",
    "process.start()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def previewCourses( dc_dict, n = 3 ):\n",
    "  crs_titles = list( dc_dict.keys() )\n",
    "  print( \"A preview of DataCamp Courses:\")\n",
    "  print(\"---------------------------------------\\n\")\n",
    "  for t in crs_titles[:n]:\n",
    "    print( \"TITLE: %s\" % t)\n",
    "    for i,ct in enumerate(dc_dict[t]):\n",
    "      print(\"\\tChapter %d: %s\" % (i+1,ct) )\n",
    "    print(\"\")\n",
    "      \n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc4aeb-7f22-48ca-9196-7f41919401a3",
   "metadata": {},
   "source": [
    "### DataCamp Descriptions\r\n",
    "Like the previous exercise, the code here is long since you are working with an entire web-crawling spider! But again, don't let the amount of code intimidate you, you have a handle on how spiders work now, and you are perfectly capable to complete the easy task for you here!\r\n",
    "\r\n",
    "As in the previous exercise, we have created a function previewCourses which lets you preview the output of the spider, but you can always just explore the dictionary dc_dict too after you run the code.\r\n",
    "\r\n",
    "In this exercise, you are asked to create a CSS Locator string direct to the text of the course description. All you need to know is that from the course page, the course description text is within a paragraph p element which belongs to the class course__description (two underlines)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae621d9b-16c2-42a3-9f80-e3b3f7c0dff3",
   "metadata": {},
   "source": [
    "Fill in the one blank below in the parse_pages method with a CSS Locator string which directs to the text within the paragraph p element which belongs to the class course__description.\r\n",
    "\n",
    "\n",
    "NOTE: If you hit Run Code, you must Reset to Sample Code to successfully use Run Code again!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b7786de-25d9-42d8-973d-5c618f5c18d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-20 16:39:39 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: scrapybot)\n",
      "2024-01-20 16:39:39 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.12.1 (tags/v3.12.1:2305ca5, Dec  7 2023, 22:03:25) [MSC v.1937 64 bit (AMD64)], pyOpenSSL 23.3.0 (OpenSSL 3.1.4 24 Oct 2023), cryptography 41.0.7, Platform Windows-11-10.0.22631-SP0\n",
      "2024-01-20 16:39:39 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-01-20 16:39:39 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-01-20 16:39:39 [scrapy.extensions.telnet] INFO: Telnet Password: 70d8d17cfe686056\n",
      "2024-01-20 16:39:39 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-01-20 16:39:39 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2024-01-20 16:39:39 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-01-20 16:39:39 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-01-20 16:39:39 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-01-20 16:39:39 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-01-20 16:39:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-01-20 16:39:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m process \u001b[38;5;241m=\u001b[39m CrawlerProcess()\n\u001b[0;32m     40\u001b[0m process\u001b[38;5;241m.\u001b[39mcrawl(DC_Description_Spider)\n\u001b[1;32m---> 41\u001b[0m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Print a preview of courses\u001b[39;00m\n\u001b[0;32m     44\u001b[0m previewCourses(dc_dict)\n",
      "File \u001b[1;32mD:\\DataCamp Courses\\Web Scraping in Python\\venv\\Lib\\site-packages\\scrapy\\crawler.py:427\u001b[0m, in \u001b[0;36mCrawlerProcess.start\u001b[1;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[0;32m    425\u001b[0m tp\u001b[38;5;241m.\u001b[39madjustPoolsize(maxthreads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mgetint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREACTOR_THREADPOOL_MAXSIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    426\u001b[0m reactor\u001b[38;5;241m.\u001b[39maddSystemEventTrigger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop)\n\u001b[1;32m--> 427\u001b[0m \u001b[43mreactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\DataCamp Courses\\Web Scraping in Python\\venv\\Lib\\site-packages\\twisted\\internet\\base.py:1317\u001b[0m, in \u001b[0;36m_SignalReactorMixin.run\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, installSignalHandlers: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartRunning\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmainLoop()\n",
      "File \u001b[1;32mD:\\DataCamp Courses\\Web Scraping in Python\\venv\\Lib\\site-packages\\twisted\\internet\\base.py:1299\u001b[0m, in \u001b[0;36m_SignalReactorMixin.startRunning\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;124;03mExtend the base implementation in order to remember whether signal\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;124;03mhandlers should be installed later.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;124;03m    installed during startup.\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_installSignalHandlers \u001b[38;5;241m=\u001b[39m installSignalHandlers\n\u001b[1;32m-> 1299\u001b[0m \u001b[43mReactorBase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartRunning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mReactorBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\DataCamp Courses\\Web Scraping in Python\\venv\\Lib\\site-packages\\twisted\\internet\\base.py:843\u001b[0m, in \u001b[0;36mReactorBase.startRunning\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    841\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorAlreadyRunning()\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_startedBefore:\n\u001b[1;32m--> 843\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorNotRestartable()\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_started \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Description_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    # Create a SelectorList of the course titles text\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    # Extract the text and strip it clean\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    # Create a SelectorList of course descriptions text\n",
    "    crs_descr = response.css( 'p.course__description::text' )\n",
    "    # Extract the text and strip it clean\n",
    "    crs_descr_ext = crs_descr.extract_first().strip()\n",
    "    # Fill in the dictionary\n",
    "    dc_dict[crs_title_ext] = crs_descr_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Description_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95848f68-fb97-4726-a405-2bee59cd633a",
   "metadata": {},
   "source": [
    "### Capstone Crawler\r\n",
    "This exercise gives you a chance to show off what you've learned! In this exercise, you will write the parse function for a spider and then fill in a few blanks to finish off the spider. On the course directory page of DataCamp, each listed course has a title and a short course description. This spider will be used to scrape the course directory to extract the course titles and short course descriptions. You will not need to follow any links this time. Everything you need to know is:\r\n",
    "\r\n",
    "The course titles are defined by the text within an h4 element whose class contains the string block__title (double underline).\r\n",
    "The short course descriptions are defined by the text within a paragraph p element whose class contains the string block__description (double underline)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4aad5b-6952-40a0-9c40-43ece211ddab",
   "metadata": {},
   "source": [
    "Assign to the variable crs_titles the extracted list of course titles on the DataCamp course directory page. You should use the contains call within your XPath, and your XPath string should point to the text of the selected objects.\r\n",
    "\n",
    "\n",
    "Assign to the variable crs_descrs the extracted list of short course descriptions. You should use the contains call within your XPath. You should use the contains call within your XPath, and your XPath string should point to the text of the selected objects.\n",
    "\n",
    "\r\n",
    "(Since we want a list of extracted data, we will use the extract() call (rather than extract_first()). )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04a07321-1a7e-4ebe-b72c-a0f6b2b1fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse method\n",
    "def parse(self, response):\n",
    "  # Extracted course titles\n",
    "  crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "  # Extracted course descriptions\n",
    "  crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "  # Fill in the dictionary: it is the spider output\n",
    "  for crs_title, crs_descr in zip(crs_titles, crs_descrs):\n",
    "    dc_dict[crs_title] = crs_descr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec89c2-eaef-45f4-b629-72bc3d559716",
   "metadata": {},
   "source": [
    "Fill in the four blanks below with the necessary entries to complete your spider.\r\n",
    "\n",
    "\n",
    "Note: If you hit Run Code, you will need to Reset to Sample before hitting Run Code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8277af6c-8329-4363-ae84-8a36510eaf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-20 17:08:47 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: scrapybot)\n",
      "2024-01-20 17:08:47 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.12.1 (tags/v3.12.1:2305ca5, Dec  7 2023, 22:03:25) [MSC v.1937 64 bit (AMD64)], pyOpenSSL 23.3.0 (OpenSSL 3.1.4 24 Oct 2023), cryptography 41.0.7, Platform Windows-11-10.0.22631-SP0\n",
      "2024-01-20 17:08:47 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-01-20 17:08:47 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-01-20 17:08:47 [scrapy.extensions.telnet] INFO: Telnet Password: 805c472ca4668cec\n",
      "2024-01-20 17:08:47 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-01-20 17:08:47 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2024-01-20 17:08:47 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-01-20 17:08:47 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-01-20 17:08:47 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-01-20 17:08:47 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-01-20 17:08:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-01-20 17:08:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m process \u001b[38;5;241m=\u001b[39m CrawlerProcess()\n\u001b[0;32m     26\u001b[0m process\u001b[38;5;241m.\u001b[39mcrawl(YourSpider)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Print a preview of courses\u001b[39;00m\n\u001b[0;32m     30\u001b[0m previewCourses(dc_dict)\n",
      "File \u001b[1;32mD:\\DataCamp Courses\\Web Scraping in Python\\venv\\Lib\\site-packages\\scrapy\\crawler.py:427\u001b[0m, in \u001b[0;36mCrawlerProcess.start\u001b[1;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[0;32m    425\u001b[0m tp\u001b[38;5;241m.\u001b[39madjustPoolsize(maxthreads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mgetint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREACTOR_THREADPOOL_MAXSIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    426\u001b[0m reactor\u001b[38;5;241m.\u001b[39maddSystemEventTrigger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop)\n\u001b[1;32m--> 427\u001b[0m \u001b[43mreactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\DataCamp Courses\\Web Scraping in Python\\venv\\Lib\\site-packages\\twisted\\internet\\base.py:1317\u001b[0m, in \u001b[0;36m_SignalReactorMixin.run\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, installSignalHandlers: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartRunning\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmainLoop()\n",
      "File \u001b[1;32mD:\\DataCamp Courses\\Web Scraping in Python\\venv\\Lib\\site-packages\\twisted\\internet\\base.py:1299\u001b[0m, in \u001b[0;36m_SignalReactorMixin.startRunning\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;124;03mExtend the base implementation in order to remember whether signal\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;124;03mhandlers should be installed later.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;124;03m    installed during startup.\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_installSignalHandlers \u001b[38;5;241m=\u001b[39m installSignalHandlers\n\u001b[1;32m-> 1299\u001b[0m \u001b[43mReactorBase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartRunning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mReactorBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\DataCamp Courses\\Web Scraping in Python\\venv\\Lib\\site-packages\\twisted\\internet\\base.py:843\u001b[0m, in \u001b[0;36mReactorBase.startRunning\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    841\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorAlreadyRunning()\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_startedBefore:\n\u001b[1;32m--> 843\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorNotRestartable()\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_started \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "  name = 'yourspider'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request(url = url_short, callback=self.parse)\n",
    "      \n",
    "  def parse(self, response):\n",
    "    # My version of the parser you wrote in the previous part\n",
    "    crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "    crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "    for crs_title, crs_descr in zip( crs_titles, crs_descrs ):\n",
    "      dc_dict[crs_title] = crs_descr\n",
    "    \n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(YourSpider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a0c24-dd13-4f85-b174-58672bad0061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
